#!/usr/bin/env sh
#!/usr/bin/env -S uv run --script --quiet
# /// script
# requires-python = "~=3.11"
# dependencies = [
#     "selenium>=4.16.0",
#     "webdriver-manager>=4.0.1",
#     "pydantic>=2.0.0",
#     "rich>=13.0.0",
#     "typer>=0.9.0",
#     "beautifulsoup4>=4.12.0",
#     "lxml>=4.9.0"
# ]
# ///

""":"
which uv >/dev/null \
    || curl -LsSf https://astral.sh/uv/install.sh | sh \
    && tail -n +3 $0 | $(head -n 2 $0 | tail -n 1 | cut -c 3-) - "$@"
exit $?
":"""

#region Documentation
"""
UV File: weed_th_scraper.uv
Purpose: Scrape cannabis shop listings from weed.th
Dependencies: Listed in script header
Usage: Just run the script directly: ./weed_th_scraper.uv
Author: Cursor AI
Date: 2024-02-04
"""
#endregion

import asyncio
import json
from pathlib import Path
from typing import List, Optional
from datetime import datetime
import time

from pydantic import BaseModel, Field
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn
import typer
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
from selenium.common.exceptions import NoSuchElementException

# Models
class ShopLocation(BaseModel):
    latitude: Optional[float] = None
    longitude: Optional[float] = None
    address: str
    city: str = Field(default="")
    country: str = Field(default="Thailand")

class Product(BaseModel):
    name: str
    grade: Optional[str] = None  # AAAA, AAA, AA etc
    thc_percentage: Optional[float] = None
    sativa_percentage: Optional[int] = None
    indica_percentage: Optional[int] = None
    description: Optional[str] = None

class Shop(BaseModel):
    id: str
    name: str
    url: str
    description: Optional[str] = None
    location: ShopLocation
    rating: Optional[float] = None
    products: List[Product] = Field(default_factory=list)
    updated_at: datetime = Field(default_factory=datetime.now)

def setup_driver():
    """Set up Chrome WebDriver with appropriate options."""
    chrome_options = Options()
    chrome_options.add_argument("--headless")  # Run in headless mode
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=chrome_options)
    return driver

def extract_shop_urls(driver: webdriver.Chrome, base_url: str, limit: int = 10) -> List[str]:
    """Extract shop URLs from the listing page."""
    driver.get(base_url)
    time.sleep(5)  # Wait for JavaScript to load
    
    # Find all shop links
    shop_links = driver.find_elements(By.CSS_SELECTOR, "a[href*='/shop/']")
    shop_urls = []
    
    for link in shop_links:
        url = link.get_attribute("href")
        if url and '/shop/' in url and not url.endswith('/shop/'):
            shop_urls.append(url)
    
    return shop_urls[:limit]

def extract_shop_details(driver: webdriver.Chrome, url: str) -> dict:
    """Extract details from a shop page."""
    driver.get(url)
    time.sleep(5)  # Wait for JavaScript to load
    
    # Extract shop ID from URL
    shop_id = url.split("/")[-1]
    
    # Extract basic shop info using WebDriverWait for better reliability
    try:
        name = WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "div[class*='Shop_header'] h1"))
        ).text
    except:
        name = url.split("/")[-1]
    
    try:
        rating_elem = WebDriverWait(driver, 5).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "div[class*='Shop_rating']"))
        )
        rating = float(rating_elem.text.strip())
    except:
        rating = None
    
    try:
        description = WebDriverWait(driver, 5).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "div[class*='Shop_description'] p"))
        ).text
    except:
        description = None
    
    # Extract location with more detailed information
    try:
        address_elem = WebDriverWait(driver, 5).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "div[class*='Shop_address'] p"))
        )
        address = address_elem.text.strip()
        
        # Try to extract coordinates from map element if available
        try:
            map_elem = driver.find_element(By.CSS_SELECTOR, "div[class*='Shop_map']")
            lat = float(map_elem.get_attribute("data-lat"))
            lng = float(map_elem.get_attribute("data-lng"))
        except:
            lat = None
            lng = None
    except:
        address = url.split("/")[-2].capitalize()
        lat = None
        lng = None
    
    city = url.split("/")[-2].capitalize()
    location = {
        "latitude": lat,
        "longitude": lng,
        "address": address,
        "city": city,
        "country": "Thailand"
    }
    
    # Extract products using the updated extract_products function
    products = extract_products(driver)
    
    return {
        "id": shop_id,
        "name": name,
        "url": url,
        "description": description,
        "location": location,
        "rating": rating,
        "products": products,
        "updated_at": datetime.now()
    }

def extract_products(driver):
    """Extract product details from the shop page."""
    products = []
    try:
        # Wait for product cards to load
        product_cards = WebDriverWait(driver, 10).until(
            EC.presence_of_all_elements_located((By.CSS_SELECTOR, "div[class*='ShopProductAll_product']"))
        )
        
        for card in product_cards:
            try:
                # Extract product details with more specific selectors
                name = card.find_element(By.CSS_SELECTOR, "div[class*='ShopProductAll_header'] a").text.strip()
                
                # Extract grade (AAAA, AAA, etc.)
                try:
                    grade = card.find_element(By.CSS_SELECTOR, "div[class*='ShopProductAll_grade']").text.strip()
                except NoSuchElementException:
                    grade = None
                
                # Extract percentages
                try:
                    thc_elem = card.find_element(By.CSS_SELECTOR, "div[class*='ShopProductAll_thc']")
                    thc_percentage = float(thc_elem.text.replace('%', '').replace('THC', '').strip())
                except:
                    thc_percentage = None
                
                try:
                    sativa_elem = card.find_element(By.CSS_SELECTOR, "div[class*='ShopProductAll_sativa']")
                    sativa_percentage = int(sativa_elem.text.replace('%', '').replace('SATIVA', '').strip())
                except:
                    sativa_percentage = None
                
                try:
                    indica_elem = card.find_element(By.CSS_SELECTOR, "div[class*='ShopProductAll_indica']")
                    indica_percentage = int(indica_elem.text.replace('%', '').replace('INDICA', '').strip())
                except:
                    indica_percentage = None
                
                # Extract description
                try:
                    description = card.find_element(By.CSS_SELECTOR, "div[class*='ShopProductAll_description'] p").text.strip()
                except:
                    description = None
                
                product = Product(
                    name=name,
                    grade=grade,
                    thc_percentage=thc_percentage,
                    sativa_percentage=sativa_percentage,
                    indica_percentage=indica_percentage,
                    description=description
                )
                products.append(product)
                
            except Exception as e:
                print(f"Error extracting product details: {str(e)}")
                continue
                
    except Exception as e:
        print(f"Error finding product cards: {str(e)}")
        
    return products

def scrape_shop_listings(limit: int = 10) -> List[Shop]:
    """Scrape shop listings using Selenium."""
    console = Console()
    shops = []
    
    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        console=console,
    ) as progress:
        task = progress.add_task("Setting up WebDriver...", total=None)
        
        try:
            # Set up WebDriver
            driver = setup_driver()
            
            # Get shop URLs
            progress.update(task, description="Getting shop listings...")
            shop_urls = extract_shop_urls(driver, "https://weed.th/shop/", limit)
            
            if not shop_urls:
                raise ValueError("No shop URLs found")
            
            progress.update(task, total=len(shop_urls))
            console.print(f"[green]Found {len(shop_urls)} shop URLs[/green]")
            
            # Scrape individual shop pages
            for url in shop_urls:
                progress.update(task, description=f"Scraping {url}...")
                try:
                    shop_data = extract_shop_details(driver, url)
                    console.print(f"[green]Found shop:[/green] {shop_data.get('name', url)}")
                    shops.append(Shop(**shop_data))
                except Exception as e:
                    console.print(f"[red]Error scraping {url}: {str(e)}[/red]")
                finally:
                    progress.advance(task)
            
            return shops
                    
        except Exception as e:
            console.print(f"[red]Error scraping shop listings: {str(e)}[/red]")
            raise
        finally:
            if 'driver' in locals():
                driver.quit()
            progress.advance(task)

def save_results(shops: List[Shop], output_file: str = "weed_th_shops.json"):
    """Save scraped shop data to a JSON file."""
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(
            [shop.model_dump(mode="json") for shop in shops],
            f,
            ensure_ascii=False,
            indent=2,
        )

def main(
    limit: int = typer.Option(10, help="Number of shops to scrape"),
    output: str = typer.Option("weed_th_shops.json", help="Output JSON file path"),
):
    """Scrape cannabis shop listings from weed.th"""
    console = Console()
    
    try:
        # Scrape shop listings
        shops = scrape_shop_listings(limit)
        
        # Save results
        save_results(shops, output)
        
        console.print(f"\n[green]Successfully scraped {len(shops)} shops![/green]")
        console.print(f"Results saved to: {output}")
        
    except Exception as e:
        console.print(f"[red]Error: {str(e)}[/red]")
        raise typer.Exit(1)

if __name__ == "__main__":
    typer.run(main) 
